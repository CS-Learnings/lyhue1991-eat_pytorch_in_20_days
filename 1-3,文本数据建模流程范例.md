# 1-3,æ–‡æœ¬æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹

```python
import os
import datetime

#æ‰“å°æ—¶é—´
def printbar():
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("\n"+"=========="*8 + "%s"%nowtime)

#macç³»ç»Ÿä¸Špytorchå’Œmatplotlibåœ¨jupyterä¸­åŒæ—¶è·‘éœ€è¦æ›´æ”¹ç¯å¢ƒå˜é‡
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE" 


```

### ä¸€ï¼Œå‡†å¤‡æ•°æ®


imdbæ•°æ®é›†çš„ç›®æ ‡æ˜¯æ ¹æ®ç”µå½±è¯„è®ºçš„æ–‡æœ¬å†…å®¹é¢„æµ‹è¯„è®ºçš„æƒ…æ„Ÿæ ‡ç­¾ã€‚

è®­ç»ƒé›†æœ‰20000æ¡ç”µå½±è¯„è®ºæ–‡æœ¬ï¼Œæµ‹è¯•é›†æœ‰5000æ¡ç”µå½±è¯„è®ºæ–‡æœ¬ï¼Œå…¶ä¸­æ­£é¢è¯„è®ºå’Œè´Ÿé¢è¯„è®ºéƒ½å„å ä¸€åŠã€‚

æ–‡æœ¬æ•°æ®é¢„å¤„ç†è¾ƒä¸ºç¹çï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ‡è¯ï¼Œæ„å»ºè¯å…¸ï¼Œç¼–ç è½¬æ¢ï¼Œåºåˆ—å¡«å……ï¼Œæ„å»ºæ•°æ®ç®¡é“ç­‰ç­‰ã€‚



åœ¨torchä¸­é¢„å¤„ç†æ–‡æœ¬æ•°æ®å¯ä»¥å€ŸåŠ©torchtextä¸­çš„è¯å…¸å·¥å…·å¹¶è‡ªå®šä¹‰Datasetã€‚

ä¸‹é¢è¿›è¡Œæ¼”ç¤ºã€‚



![](./data/ç”µå½±è¯„è®º.jpg)

```python
!pip install torchtext 
```

```python
import numpy as np 
import pandas as pd 
import torch 
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator

MAX_WORDS = 10000  #ä»…è€ƒè™‘æœ€é«˜é¢‘çš„10000ä¸ªè¯
MAX_LEN = 200      #æ¯ä¸ªæ ·æœ¬ä¿ç•™200ä¸ªè¯çš„é•¿åº¦
BATCH_SIZE = 20 



dftrain = pd.read_csv("./data/imdb/train.tsv",sep="\t",header = None,names = ["label","text"])
dfval = pd.read_csv("./data/imdb/test.tsv",sep="\t",header = None,names = ["label","text"])

#1ï¼Œæ–‡æœ¬åˆ‡è¯
tokenizer = get_tokenizer('basic_english')


#2ï¼Œæ„å»ºè¯å…¸        
PAD_IDX,UNK_IDX = 0,1
special_symbols = ['<pad>','<unk>']

def yield_tokens(dfdata):
    for text in dfdata["text"]:
        yield tokenizer(text)
        
vocab = build_vocab_from_iterator(
    yield_tokens(dftrain),
    max_tokens = MAX_WORDS,
    specials=special_symbols,
    special_first=True)

vocab.set_default_index(UNK_IDX)

#æŸ¥çœ‹è¯å…¸å‰20ä¸ªè¯
#itos: index to string
#stoi: string to index
print("vocab.get_itos():\n",vocab.get_itos()[:20])
print("vocab.get_stoi()['<pad>']:\n",vocab.get_stoi()['<pad>'])


#3ï¼Œåºåˆ—å¡«å……
def pad(seq,max_length,pad_value=0):
    n = len(seq)
    result = seq+[pad_value]*max_length
    return result[:max_length]


#4ï¼Œç¼–ç è½¬æ¢
def text_pipeline(text):
    words = tokenizer(text)
    tokens = vocab(words)
    result = pad(tokens,MAX_LEN,PAD_IDX)
    return result 

print(text_pipeline("this is an example!")) 

```

```python
#5ï¼Œæ„å»ºç®¡é“
from torch.utils.data import Dataset,DataLoader

class ImdbDataset(Dataset):
    def __init__(self,df):
        self.df = df
    def __len__(self):
        return len(self.df)
    def __getitem__(self,index):
        text = self.df["text"].iloc[index]
        label = torch.tensor(self.df["label"].iloc[index]).float()
        tokens = torch.tensor(text_pipeline(text)).int() 
        return tokens,label
    
ds_train = ImdbDataset(dftrain)
ds_val = ImdbDataset(dfval)

```

```python
dl_train = DataLoader(ds_train,batch_size = 50,shuffle = True)
dl_val = DataLoader(ds_val,batch_size = 50,shuffle = False)

```

```python
for features,labels in dl_train:
    break 
```

```python

```

### äºŒï¼Œå®šä¹‰æ¨¡å‹


ä½¿ç”¨Pytorché€šå¸¸æœ‰ä¸‰ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼šä½¿ç”¨nn.SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ï¼Œç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ï¼Œç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºæ¨¡å‹å¹¶è¾…åŠ©åº”ç”¨æ¨¡å‹å®¹å™¨(nn.Sequential,nn.ModuleList,nn.ModuleDict)è¿›è¡Œå°è£…ã€‚

æ­¤å¤„é€‰æ‹©ä½¿ç”¨ç¬¬ä¸‰ç§æ–¹å¼è¿›è¡Œæ„å»ºã€‚


```python
import torch
from torch import nn 
torch.manual_seed(42)

```

```python
class Net(nn.Module):
    
    def __init__(self):
        super(Net, self).__init__()
        
        #è®¾ç½®padding_idxå‚æ•°åå°†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†å¡«å……çš„tokenå§‹ç»ˆèµ‹å€¼ä¸º0å‘é‡
        self.embedding = nn.Embedding(num_embeddings = MAX_WORDS,embedding_dim = 3,padding_idx = 0)
        
        self.conv = nn.Sequential()
        self.conv.add_module("conv_1",nn.Conv1d(in_channels = 3,out_channels = 16,kernel_size = 5))
        self.conv.add_module("pool_1",nn.MaxPool1d(kernel_size = 2))
        self.conv.add_module("relu_1",nn.ReLU())
        self.conv.add_module("conv_2",nn.Conv1d(in_channels = 16,out_channels = 128,kernel_size = 2))
        self.conv.add_module("pool_2",nn.MaxPool1d(kernel_size = 2))
        self.conv.add_module("relu_2",nn.ReLU())
        
        self.dense = nn.Sequential()
        self.dense.add_module("flatten",nn.Flatten())
        self.dense.add_module("linear",nn.Linear(6144,1))
        
        
    def forward(self,x):
        x = self.embedding(x).transpose(1,2)
        x = self.conv(x)
        y = self.dense(x)
        return y
        

def create_net():
    return Net()

net = create_net() 
print(net)

#from torchkeras import summary 

#summary(features,input_data=features)

```

```python

```

### ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹


è®­ç»ƒPytorché€šå¸¸éœ€è¦ç”¨æˆ·ç¼–å†™è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œè®­ç»ƒå¾ªç¯çš„ä»£ç é£æ ¼å› äººè€Œå¼‚ã€‚

æœ‰3ç±»å…¸å‹çš„è®­ç»ƒå¾ªç¯ä»£ç é£æ ¼ï¼šè„šæœ¬å½¢å¼è®­ç»ƒå¾ªç¯ï¼Œå‡½æ•°å½¢å¼è®­ç»ƒå¾ªç¯ï¼Œç±»å½¢å¼è®­ç»ƒå¾ªç¯ã€‚

æ­¤å¤„ä»‹ç»ä¸€ç§è¾ƒé€šç”¨çš„ä»¿ç…§Kerasé£æ ¼çš„ç±»å½¢å¼çš„è®­ç»ƒå¾ªç¯ã€‚

è¯¥è®­ç»ƒå¾ªç¯çš„ä»£ç ä¹Ÿæ˜¯torchkerasåº“çš„æ ¸å¿ƒä»£ç ã€‚

torchkerasè¯¦æƒ…:  https://github.com/lyhue1991/torchkeras 



```python
import os,sys,time
import numpy as np
import pandas as pd
import datetime 
from tqdm import tqdm 

import torch
from torch import nn 
from copy import deepcopy

def printlog(info):
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("\n"+"=========="*8 + "%s"%nowtime)
    print(str(info)+"\n")

class StepRunner:
    def __init__(self, net, loss_fn,stage = "train", metrics_dict = None, 
                 optimizer = None, lr_scheduler = None
                 ):
        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage
        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler
    
    def __call__(self, features, labels):
        #loss
        preds = self.net(features)
        loss = self.loss_fn(preds,labels)

        #backward()
        if self.optimizer is not None and self.stage=="train":
            loss.backward()
            self.optimizer.step()
            if self.lr_scheduler is not None:
                self.lr_scheduler.step()
            self.optimizer.zero_grad()
            
        #metrics
        step_metrics = {self.stage+"_"+name:metric_fn(preds, labels).item() 
                        for name,metric_fn in self.metrics_dict.items()}
        return loss.item(),step_metrics


class EpochRunner:
    def __init__(self,steprunner):
        self.steprunner = steprunner
        self.stage = steprunner.stage
        self.steprunner.net.train() if self.stage=="train" else self.steprunner.net.eval()
        
    def __call__(self,dataloader):
        total_loss,step = 0,0
        loop = tqdm(enumerate(dataloader), total =len(dataloader))
        for i, batch in loop: 
            if self.stage=="train":
                loss, step_metrics = self.steprunner(*batch)
            else:
                with torch.no_grad():
                    loss, step_metrics = self.steprunner(*batch)
            step_log = dict({self.stage+"_loss":loss},**step_metrics)

            total_loss += loss
            step+=1
            if i!=len(dataloader)-1:
                loop.set_postfix(**step_log)
            else:
                epoch_loss = total_loss/step
                epoch_metrics = {self.stage+"_"+name:metric_fn.compute().item() 
                                 for name,metric_fn in self.steprunner.metrics_dict.items()}
                epoch_log = dict({self.stage+"_loss":epoch_loss},**epoch_metrics)
                loop.set_postfix(**epoch_log)

                for name,metric_fn in self.steprunner.metrics_dict.items():
                    metric_fn.reset()
        return epoch_log

class KerasModel(torch.nn.Module):
    def __init__(self,net,loss_fn,metrics_dict=None,optimizer=None,lr_scheduler = None):
        super().__init__()
        self.history = {}
        
        self.net = net
        self.loss_fn = loss_fn
        self.metrics_dict = nn.ModuleDict(metrics_dict) 
        
        self.optimizer = optimizer if optimizer is not None else torch.optim.Adam(
            self.parameters(), lr=1e-2)
        self.lr_scheduler = lr_scheduler
        
        self.net,self.loss_fn,self.metrics_dict,self.optimizer = self.accelerator.prepare(
            self.net,self.loss_fn,self.metrics_dict,self.optimizer)

    def forward(self, x):
        if self.net:
            return self.net.forward(x)
        else:
            raise NotImplementedError


    def fit(self, train_data, val_data=None, epochs=10, ckpt_path='checkpoint.pt', 
            patience=5, monitor="val_loss", mode="min"):


        for epoch in range(1, epochs+1):
            printlog("Epoch {0} / {1}".format(epoch, epochs))
            
            # 1ï¼Œtrain -------------------------------------------------  
            train_step_runner = StepRunner(net = self.net,stage="train",
                    loss_fn = self.loss_fn,metrics_dict=deepcopy(self.metrics_dict),
                    optimizer = self.optimizer, lr_scheduler = self.lr_scheduler)
            train_epoch_runner = EpochRunner(train_step_runner)
            train_metrics = train_epoch_runner(train_data)
            
            for name, metric in train_metrics.items():
                self.history[name] = self.history.get(name, []) + [metric]

            # 2ï¼Œvalidate -------------------------------------------------
            if val_data:
                val_step_runner = StepRunner(net = self.net,stage="val",
                    loss_fn = self.loss_fn,metrics_dict=deepcopy(self.metrics_dict))
                val_epoch_runner = EpochRunner(val_step_runner)
                with torch.no_grad():
                    val_metrics = val_epoch_runner(val_data)
                val_metrics["epoch"] = epoch
                for name, metric in val_metrics.items():
                    self.history[name] = self.history.get(name, []) + [metric]
            
            # 3ï¼Œearly-stopping -------------------------------------------------
            arr_scores = self.history[monitor]
            best_score_idx = np.argmax(arr_scores) if mode=="max" else np.argmin(arr_scores)
            if best_score_idx==len(arr_scores)-1:
                torch.save(self.net.state_dict(),ckpt_path)
                print("<<<<<< reach best {0} : {1} >>>>>>".format(monitor,
                     arr_scores[best_score_idx]),file=sys.stderr)
            if len(arr_scores)-best_score_idx>patience:
                print("<<<<<< {} without improvement in {} epoch, early stopping >>>>>>".format(
                    monitor,patience),file=sys.stderr)
                self.net.load_state_dict(torch.load(ckpt_path))
                break 
            
        return pd.DataFrame(self.history)

    @torch.no_grad()
    def evaluate(self, val_data):
        val_step_runner = StepRunner(net = self.net,stage="val",
                    loss_fn = self.loss_fn,metrics_dict=deepcopy(self.metrics_dict))
        val_epoch_runner = EpochRunner(val_step_runner)
        val_metrics = val_epoch_runner(val_data)
        return val_metrics
        
       
    @torch.no_grad()
    def predict(self, dataloader):
        self.net.eval()
        result = torch.cat([self.forward(t[0]) for t in dataloader])
        return result.data
    
```

```python
import torchmetrics 

class Accuracy(torchmetrics.Accuracy):
    def __init__(self, dist_sync_on_step=False):
        super().__init__(dist_sync_on_step=dist_sync_on_step)
        
    def update(self, preds: torch.Tensor, targets: torch.Tensor):
        super().update(torch.sigmoid(preds),targets.long())
            
    def compute(self):
        return super().compute()

model = KerasModel(net,
                  loss_fn = nn.BCEWithLogitsLoss(),
                  optimizer= torch.optim.Adam(net.parameters(),lr = 0.03),
                  metrics_dict = {"acc":Accuracy()}
                )

```

```python

```

```python

```

```python

```

```python

```

```python

```

### å››ï¼Œè¯„ä¼°æ¨¡å‹

```python
import pandas as pd 

history = model.history
dfhistory = pd.DataFrame(history) 
dfhistory 
```

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(dfhistory, metric):
    train_metrics = dfhistory[metric]
    val_metrics = dfhistory['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()
```

```python
plot_metric(dfhistory,"loss")
```

![](./data/1-3-lossæ›²çº¿.png)

```python
plot_metric(dfhistory,"accuracy")
```

![](./data/1-3-accuracyæ›²çº¿.png)

```python
# è¯„ä¼°
results = trainer.test(model, test_dataloaders=dl_valid, verbose = False)
print(results[0])

```

```
{'val_loss': 0.5056138457655907, 'val_accuracy': 0.7948000040054322}
```


### äº”ï¼Œä½¿ç”¨æ¨¡å‹

```python
def predict(model,dl):
    model.eval()
    result = torch.cat([model.forward(t[0].to(model.device)) for t in dl])
    return(result.data)

result = predict(model,dl_valid)
result 
```

```
tensor([[0.0357],
        [0.8699],
        [0.3303],
        ...,
        [0.9962],
        [0.5566],
        [0.0491]])
```

```python

```

### å…­ï¼Œä¿å­˜æ¨¡å‹

```python
print(ckpt_cb.best_model_score)
model.load_from_checkpoint(ckpt_cb.best_model_path)

best_net  = model.net
torch.save(best_net.state_dict(),"./data/net.pt")

```

```python
net_clone = Net()
net_clone.load_state_dict(torch.load("./data/net.pt"))
model_clone = Model(net_clone)
trainer = pl.Trainer()
result = trainer.test(model_clone,test_dataloaders=dl_valid, verbose = False) 

print(result)
```

```
[{'test_loss': 0.4958915710449219, 'test_accuracy': 0.75}]
```


**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"ç®—æ³•ç¾é£Ÿå±‹"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**åŠ ç¾¤**ï¼ŒåŠ å…¥è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

![ç®—æ³•ç¾é£Ÿå±‹logo.png](https://tva1.sinaimg.cn/large/e6c9d24egy1h41m2zugguj20k00b9q46.jpg)
